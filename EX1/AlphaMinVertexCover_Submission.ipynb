{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    \"Monte Carlo tree searcher. First rollout the tree then choose a move.\"\n",
    "\n",
    "    def __init__(self, exploration_weight=1):\n",
    "        self.Q = defaultdict(int)  # total reward of each node\n",
    "        self.N = defaultdict(int)  # total visit count for each node\n",
    "        self.children = dict()  # children of each node\n",
    "        self.exploration_weight = exploration_weight\n",
    "\n",
    "    def choose(self, node):\n",
    "        \"Choose the best successor of node. (Choose a move in the game)\"\n",
    "        if node.is_terminal():\n",
    "            raise RuntimeError(f\"choose called on terminal node {node}\")\n",
    "\n",
    "        if node not in self.children:\n",
    "            return node.find_random_child()\n",
    "\n",
    "        def score(n):\n",
    "            if self.N[n] == 0:\n",
    "                return float(\"-inf\")  # avoid unseen moves\n",
    "            return self.Q[n] / self.N[n]  # average reward\n",
    "\n",
    "        return max(self.children[node], key=score)\n",
    "\n",
    "    def do_rollout(self, node):\n",
    "        \"Make the tree one layer better. (Train for one iteration.)\"\n",
    "        path = self._select(node)\n",
    "        leaf = path[-1]\n",
    "        self._expand(leaf)\n",
    "        reward = self._simulate(leaf)\n",
    "        self._backpropagate(path, reward)\n",
    "\n",
    "    def _select(self, node):\n",
    "        \"Find an unexplored descendent of `node`\"\n",
    "        path = []\n",
    "        while True:\n",
    "            path.append(node)\n",
    "            if node not in self.children or not self.children[node]:\n",
    "                # node is either unexplored or terminal\n",
    "                return path\n",
    "            unexplored = self.children[node] - self.children.keys()\n",
    "            if unexplored:\n",
    "                n = unexplored.pop()\n",
    "                path.append(n)\n",
    "                return path\n",
    "            node = self._uct_select(node)  # descend a layer deeper\n",
    "\n",
    "    def _expand(self, node):\n",
    "        \"Update the `children` dict with the children of `node`\"\n",
    "        if node in self.children:\n",
    "            return  # already expanded\n",
    "        self.children[node] = node.find_children()\n",
    "\n",
    "    def _simulate(self, node):\n",
    "        \"Returns the reward for a random simulation (to completion) of `node`\"\n",
    "        #invert_reward = True\n",
    "        reward = 0\n",
    "        while True:\n",
    "            #print(node.to_pretty_string())\n",
    "            if node.is_terminal():\n",
    "                return reward\n",
    "            node = node.find_random_child()\n",
    "            reward = node.get_reward()\n",
    "\n",
    "    def _backpropagate(self, path, reward):\n",
    "        \"Send the reward back up to the ancestors of the leaf\"\n",
    "        for node in reversed(path):\n",
    "            self.N[node] += 1\n",
    "            self.Q[node] += reward\n",
    "            #reward = 1 - reward  # 1 for me is 0 for my enemy, and vice versa\n",
    "\n",
    "    def _uct_select(self, node):\n",
    "        \"Select a child of node, balancing exploration & exploitation\"\n",
    "\n",
    "        # All children of node should already be expanded:\n",
    "        assert all(n in self.children for n in self.children[node])\n",
    "\n",
    "        log_N_vertex = math.log(self.N[node])\n",
    "\n",
    "        def uct(n):\n",
    "            \"Upper confidence bound for trees\"\n",
    "            return self.Q[n] / self.N[n] + self.exploration_weight * math.sqrt(\n",
    "                log_N_vertex / self.N[n]\n",
    "            )\n",
    "\n",
    "        return max(self.children[node], key=uct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(ABC):\n",
    "    \"\"\"\n",
    "    A representation of a single board state.\n",
    "    MCTS works by constructing a tree of these Nodes.\n",
    "    Could be e.g. a chess or checkers board state.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def find_children(self):\n",
    "        \"All possible successors of this board state\"\n",
    "        return set()\n",
    "\n",
    "    @abstractmethod\n",
    "    def find_random_child(self):\n",
    "        \"Random successor of this board state (for more efficient simulation)\"\n",
    "        return None\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_terminal(self):\n",
    "        \"Returns True if the node has no children\"\n",
    "        return True\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def reward(self):\n",
    "#         \"Assumes `self` is terminal node. 1=win, 0=loss, .5=tie, etc\"\n",
    "#         return 0\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def __hash__(self):\n",
    "#         \"Nodes must be hashable\"\n",
    "#         return 123456789\n",
    "\n",
    "#     @abstractmethod\n",
    "#     def __eq__(node1, node2):\n",
    "#         \"Nodes must be comparable\"\n",
    "#         return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random graph of preset size\n",
    "def generate_graph(vertices):\n",
    "    return nx.generators.random_graphs.gnp_random_graph(vertices,\n",
    "                np.random.uniform(0,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertexCoverInstance(Node):\n",
    "    def __init__(self, graph, cover = [], reward = 0):\n",
    "        self.graph = graph\n",
    "        self.cover = cover\n",
    "        self.reward = reward\n",
    "        self.possible_moves = [node for node in list(graph.nodes) if graph.degree[node]]\n",
    "        \n",
    "    def find_children(self):\n",
    "        possiblemoves = []\n",
    "        possiblenodes = []\n",
    "        if self.is_terminal():  # If the game is finished then no moves can be made\n",
    "            return possiblemoves\n",
    "        for i in list(self.graph.nodes):\n",
    "            H = self.graph.copy()\n",
    "            if H.degree[i]:\n",
    "                step_reward = -1\n",
    "                H.remove_node(i)\n",
    "                H.add_node(i)\n",
    "                possiblenodes.append(i)\n",
    "                possiblemoves.append(VertexCoverInstance(H, self.cover+[i], self.reward+step_reward))\n",
    "        return possiblemoves\n",
    "\n",
    "    def find_children_with_actions(self):\n",
    "        possiblemoves = []\n",
    "        possiblenodes = []\n",
    "        if self.is_terminal():  # If the game is finished then no moves can be made\n",
    "            return (possiblenodes, possiblemoves)\n",
    "        for i in list(self.graph.nodes):\n",
    "            H = self.graph.copy()\n",
    "            if H.degree[i]:\n",
    "                step_reward = -1\n",
    "                H.remove_node(i)\n",
    "                H.add_node(i)\n",
    "                possiblenodes.append(i)\n",
    "                possiblemoves.append(VertexCoverInstance(H, self.cover+[i], self.reward+step_reward))\n",
    "        return (possiblenodes, possiblemoves)\n",
    "        \n",
    "    \n",
    "    def take_action(self, action):\n",
    "        if self.is_terminal():\n",
    "            raise IndexError(\"Terminal state cannot be acted upon\")\n",
    "        if action not in self.possible_moves:\n",
    "            raise IndexError(\"This action is illegal\")\n",
    "        H = self.graph.copy()\n",
    "        step_reward = -1\n",
    "        H.remove_node(action)\n",
    "        H.add_node(action)\n",
    "        return VertexCoverInstance(H, self.cover+[action], self.reward+step_reward)\n",
    "\n",
    "    def find_random_child(self):\n",
    "        if self.is_terminal():\n",
    "            return None  # If the game is finished then no moves can be made\n",
    "        temp = self.find_children()\n",
    "        return random.sample(set(temp),1)[0]\n",
    "\n",
    "#     def reward(board):\n",
    "#         if not board.terminal:\n",
    "#             raise RuntimeError(f\"reward called on nonterminal board {board}\")\n",
    "#         return self.reward #reward comes upon reaching terminal state\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return nx.classes.function.is_empty(self.graph)\n",
    "\n",
    "    def to_pretty_string(self):\n",
    "        return str(list(self.graph.nodes()))\n",
    "    \n",
    "    def get_cover(self):\n",
    "        return self.cover\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return self.reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(G, display = True):\n",
    "    tree = MCTS()\n",
    "    board = VertexCoverInstance(G)\n",
    "    #print(board.to_pretty_string())\n",
    "    moves = 0\n",
    "    while True:\n",
    "        if board.is_terminal():\n",
    "            break\n",
    "        #80 rollouts per turn\n",
    "        for _ in range(80):\n",
    "            tree.do_rollout(board)\n",
    "        board = tree.choose(board)\n",
    "        #print(board.to_pretty_string())\n",
    "        moves = moves+1\n",
    "        if board.is_terminal():\n",
    "            break\n",
    "            \n",
    "    vc = board.get_cover()\n",
    "    if display:\n",
    "        print(\"\\n\\nMCTS APPROX\\n\")\n",
    "        print(vc)\n",
    "        color_map = []\n",
    "        for node in G:\n",
    "            if node in vc:\n",
    "                color_map.append('red')\n",
    "            else: \n",
    "                color_map.append('blue')  \n",
    "        plt.figure(1)\n",
    "        nx.draw(G, node_color=color_map, with_labels=True)\n",
    "        \n",
    "    return vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import approximation as appx\n",
    "import itertools\n",
    "\n",
    "def show_optimal_vc(G, display=True):\n",
    "    #opt = list(appx.vertex_cover.min_weighted_vertex_cover(G))\n",
    "    def findsubsets(s): \n",
    "        lists =[list(itertools.combinations(s, n)) for n in range(len(s))]\n",
    "        return list(itertools.chain.from_iterable(lists))\n",
    "    \n",
    "    powerset = findsubsets(list(G.nodes()))    \n",
    "    for s in powerset:\n",
    "        H = G.copy()\n",
    "        H.remove_nodes_from(s)\n",
    "        if nx.classes.function.is_empty(H):\n",
    "            opt = list(s)\n",
    "            break\n",
    "    if display:\n",
    "        print(\"\\n\\nOPTIMAL\\n\")\n",
    "        print(opt)\n",
    "        color_map = []\n",
    "        for node in G:\n",
    "            if node in opt:\n",
    "                color_map.append('orange')\n",
    "            else: \n",
    "                color_map.append('green')\n",
    "        plt.figure(2)\n",
    "        nx.draw(G, node_color=color_map, with_labels=True)\n",
    "        \n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a33efeb3d7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_optimal_vc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-962f7bec6345>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(G, display)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#80 rollouts per turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print(board.to_pretty_string())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-94809fa0687e>\u001b[0m in \u001b[0;36mdo_rollout\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mleaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-94809fa0687e>\u001b[0m in \u001b[0;36m_simulate\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_random_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-14177c8eef4b>\u001b[0m in \u001b[0;36mfind_random_child\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# If the game is finished then no moves can be made\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-14177c8eef4b>\u001b[0m in \u001b[0;36mfind_children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpossiblemoves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mstep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/columbia-deep-learning-project/env/lib/python3.7/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, as_view)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         G.add_edges_from((u, v, datadict.copy())\n\u001b[0;32m-> 1515\u001b[0;31m                          \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbrs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1516\u001b[0m                          for v, datadict in nbrs.items())\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/columbia-deep-learning-project/env/lib/python3.7/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36madd_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    947\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_attr_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0mdatadict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0mdatadict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m             \u001b[0mdatadict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "G = generate_graph(20)\n",
    "vc = play_game(G)\n",
    "print()\n",
    "opt = show_optimal_vc(G)\n",
    "print('Finished in {} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SIMS = 1000\n",
    "\n",
    "def sim_test_approx(simulations):\n",
    "    count_success = 0\n",
    "    counts = []\n",
    "    for x in range(simulations):\n",
    "        A = generate_graph(10)\n",
    "        vc = play_game(A, display=False)\n",
    "        opt = show_optimal_vc(A, display=False)\n",
    "        if len(vc)<=len(opt)*1.1:\n",
    "            count_success=count_success+1\n",
    "        counts.append(count_success)\n",
    "    return np.array(counts), count_success\n",
    "\n",
    "counts, successes = sim_test_approx(NUM_SIMS)\n",
    "#plot_arr = [successes, NUM_SIMS-successes]\n",
    "plt.figure(3)\n",
    "#plt.pie(plot_arr, colors = ['lightcoral','lightskyblue'], labels = ['good approx', 'bad approx'],autopct='%1.2f%%')\n",
    "plt.plot(counts/np.array(range(1,NUM_SIMS+1)))\n",
    "plt.xlabel('Number of Simulations')\n",
    "plt.ylabel('Success Percentage')\n",
    "plt.title('Percentage of Good MCTS Approximations (Min Vertex Cover)')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS_AZ:\n",
    "    \"Monte Carlo tree searcher with GNN. First rollout the tree then choose a move.\"\n",
    "\n",
    "    def __init__(self, net, exploration_weight=1):\n",
    "        self.Q = defaultdict(int)  # total reward of each node\n",
    "        self.N = defaultdict(int)  # total visit count for each node\n",
    "        self.children = dict()  # children of each node\n",
    "        self.priors = dict() # prior probability of visiting each child of a given node\n",
    "        self.exploration_weight = exploration_weight\n",
    "        self.net = net\n",
    "\n",
    "    def choose(self, node):\n",
    "        \"Choose the best successor of node. (Choose a move in the game)\"\n",
    "        if node.is_terminal():\n",
    "            raise RuntimeError(f\"choose called on terminal node {node}\")\n",
    "\n",
    "        if node not in self.children:\n",
    "            return node.find_random_child()\n",
    "\n",
    "        def score(n):\n",
    "            if self.N[n] == 0:\n",
    "                return float(\"-inf\")  # avoid unseen moves\n",
    "            return self.Q[n] / self.N[n]  # average reward\n",
    "\n",
    "        return max(self.children[node], key=score)\n",
    "    \n",
    "    def choose_by_policy(self, node):\n",
    "        \"Choose a successor of node according to policy\"\n",
    "        if node.is_terminal():\n",
    "            raise RuntimeError(f\"choose called on terminal node {node}\")\n",
    "            \n",
    "        if node not in self.children:\n",
    "            return node.find_random_child()\n",
    "        \n",
    "        policy = list(map(lambda n: self.N[n]/(self.N[node]-1), self.children[node]))\n",
    "        action = np.random.choice(len(policy), 1, p=policy)[0]\n",
    "        successor = self.children[node][action]\n",
    "\n",
    "        return successor,policy\n",
    "\n",
    "    def do_rollout(self, node):\n",
    "        \"Make the tree one layer better. (Train for one iteration.)\"\n",
    "        path = self._select(node)\n",
    "        leaf = path[-1]\n",
    "        self._expand(leaf)\n",
    "        reward = self._simulate(leaf)\n",
    "        self._backpropagate(path, reward)\n",
    "\n",
    "    def _select(self, node):\n",
    "        \"Find an unexplored descendent of `node`\"\n",
    "        path = []\n",
    "        while True:\n",
    "            path.append(node)\n",
    "            if node not in self.children or not self.children[node]:\n",
    "                # node is either unexplored or terminal\n",
    "                return path\n",
    "            for child in self.children[node]:\n",
    "                if child not in self.children:\n",
    "                    path.append(child)\n",
    "                    return path\n",
    "            node = self._uct_select(node)  # descend a layer deeper\n",
    "\n",
    "    def _expand(self, node):\n",
    "        \"Update the `children` dict with the children of `node`\"\n",
    "        if node in self.children:\n",
    "            return  # already expanded\n",
    "        vertices, children = node.find_children_with_actions()\n",
    "        self.children[node] = children\n",
    "        if children:\n",
    "            all_priors = self.net.predict(node)\n",
    "            all_priors *= len(all_priors)\n",
    "            self.priors[node] = [all_priors[i] for i in vertices]\n",
    "\n",
    "    def _simulate(self, node):\n",
    "        \"Returns the reward for a random simulation (to completion) of `node`\"\n",
    "        reward = 0\n",
    "        while True:\n",
    "            if node.is_terminal():\n",
    "                return reward\n",
    "            node = node.find_random_child()\n",
    "            reward = node.get_reward()\n",
    "\n",
    "    def _backpropagate(self, path, reward):\n",
    "        \"Send the reward back up to the ancestors of the leaf\"\n",
    "        for node in reversed(path):\n",
    "            self.N[node] += 1\n",
    "            self.Q[node] += reward\n",
    "\n",
    "    def _uct_select(self, node):\n",
    "        \"Select a child of node, balancing exploration & exploitation\"\n",
    "\n",
    "        # All children of node should already be expanded:\n",
    "        assert all(n in self.children for n in self.children[node])\n",
    "\n",
    "        log_N_vertex = math.log(self.N[node])\n",
    "\n",
    "        def uct(n):\n",
    "            \"Upper confidence bound for trees\"\n",
    "            i,n = n # expand from enumerate tuple\n",
    "            return self.Q[n] / self.N[n] + self.exploration_weight * self.priors[node][i] * math.sqrt(\n",
    "                self.N[node] / self.N[n]\n",
    "            )\n",
    "#             return self.Q[n] / self.N[n] + self.exploration_weight * math.sqrt(\n",
    "#                  log_N_vertex / self.N[n]\n",
    "#             )\n",
    "\n",
    "        return max(enumerate(self.children[node]), key=uct)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game_AZ(G, net,num_rollouts=80, display = True, net_weight = 1):\n",
    "    tree = MCTS_AZ(net, exploration_weight = net_weight)\n",
    "    board = VertexCoverInstance(G)\n",
    "    while True:\n",
    "        if board.is_terminal():\n",
    "            break\n",
    "        for _ in range(num_rollouts):\n",
    "            tree.do_rollout(board)\n",
    "        board = tree.choose(board)\n",
    "            \n",
    "    vc = board.get_cover()\n",
    "    if display:\n",
    "        print(\"\\n\\nMCTS APPROX\\n\")\n",
    "        print(vc)\n",
    "        color_map = []\n",
    "        for node in G:\n",
    "            if node in vc:\n",
    "                color_map.append('red')\n",
    "            else: \n",
    "                color_map.append('blue')  \n",
    "        plt.figure(1)\n",
    "        nx.draw(G, node_color=color_map, with_labels=True)\n",
    "        \n",
    "    return vc\n",
    "\n",
    "def play_game_net(G, net, display=True):\n",
    "    board = VertexCoverInstance(G)\n",
    "    while not board.is_terminal():\n",
    "        pred_policy = net.predict(board)\n",
    "        updated_policy = []\n",
    "        count = 0\n",
    "        for i in range(board.graph.number_of_nodes()):\n",
    "            if i in board.possible_moves:\n",
    "                updated_policy.append(pred_policy[count])\n",
    "                count+=1\n",
    "            else:\n",
    "                updated_policy.append(0)\n",
    "        \n",
    "        #print(updated_policy)\n",
    "        action = updated_policy.index(max(updated_policy))\n",
    "        #print(action)\n",
    "        board = board.take_action(action)\n",
    "        #print(board.get_cover())\n",
    "    vc = board.get_cover()\n",
    "    \n",
    "    if display:\n",
    "        print(\"\\n\\nNNET APPROX\\n\")\n",
    "        print(vc)\n",
    "        color_map = []\n",
    "        for node in G:\n",
    "            if node in vc:\n",
    "                color_map.append('red')\n",
    "            else: \n",
    "                color_map.append('blue')  \n",
    "        plt.figure(1)\n",
    "        nx.draw(G, node_color=color_map, with_labels=True)\n",
    "        \n",
    "    return vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play(G, net, num_rollouts=80, net_weight = 1):\n",
    "    tree = MCTS_AZ(net, exploration_weight = net_weight)\n",
    "    board = VertexCoverInstance(G)\n",
    "    init_vertices = board.graph.number_of_nodes()\n",
    "    data = []\n",
    "    while True:\n",
    "        if board.is_terminal():\n",
    "            break\n",
    "        for _ in range(num_rollouts):\n",
    "            tree.do_rollout(board)\n",
    "        \n",
    "        possibilities = board.possible_moves\n",
    "        new_board, policy = tree.choose_by_policy(board)\n",
    "        updated_policy = []\n",
    "        count = 0\n",
    "        for i in range(init_vertices):\n",
    "            if i in possibilities:\n",
    "                updated_policy.append(policy[count])\n",
    "                count+=1\n",
    "            else:\n",
    "                updated_policy.append(0)\n",
    "        \n",
    "        record = from_networkx(board.graph)\n",
    "        record.y = torch.tensor(updated_policy).reshape(1,-1)\n",
    "        data.append(record)\n",
    "        board = new_board \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(1, 8)\n",
    "        self.conv2 = GCNConv(8, 16)\n",
    "        self.conv3 = GCNConv(16, 16)\n",
    "        self.conv4 = GCNConv(16, 8)\n",
    "        self.conv_prob = GCNConv(8, 1)\n",
    "\n",
    "    # expects a torch_geometric Data object\n",
    "    def forward(self, data):\n",
    "        num_nodes = data.num_nodes\n",
    "        x = torch.tensor([[1.0] for _ in range(num_nodes)], requires_grad=True)\n",
    "        edge_index = data.edge_index\n",
    "        x = self.conv1(x ,edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv_prob(x, edge_index)\n",
    "        if hasattr(data, 'batch'):\n",
    "            num_nodes = len(list(filter(lambda x: x == 0, data.batch)))\n",
    "        x = x.view(-1,num_nodes)\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def predict(self, instance):\n",
    "        data = from_networkx(instance.graph)\n",
    "        return self.forward(data).flatten().tolist()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 9]\n",
      "[0.03773584905660377, 0.031446540880503145, 0.5974842767295597, 0.012578616352201259, 0.018867924528301886, 0.03773584905660377, 0.031446540880503145, 0.050314465408805034, 0.18238993710691823]\n",
      "Iteration 0\n",
      "Getting self-play records\n",
      "  Generated game 4000\n",
      "  Generated game 8000\n",
      "  Generated game 12000\n",
      "  Generated game 16000\n",
      "  Generated game 20000\n",
      "Training\n",
      "Epoch 0 loss: 0.318\n",
      "Epoch 2 loss: 0.311\n",
      "Epoch 4 loss: 0.311\n",
      "Epoch 6 loss: 0.311\n",
      "Epoch 8 loss: 0.310\n",
      "[0.07839347422122955, 0.2886858582496643, 0.1705552339553833, 0.12144291400909424, 0.0345490463078022, 0.0338842011988163, 0.03950045257806778, 0.03753327950835228, 0.08447989076375961, 0.11097568273544312]\n",
      "Evaluating\n",
      "Score: 0.415\n",
      "Iteration 1\n",
      "Getting self-play records\n",
      "  Generated game 4000\n",
      "  Generated game 8000\n",
      "  Generated game 12000\n",
      "  Generated game 16000\n",
      "  Generated game 20000\n",
      "Training\n",
      "Epoch 0 loss: 0.297\n",
      "Epoch 2 loss: 0.297\n",
      "Epoch 4 loss: 0.297\n",
      "Epoch 6 loss: 0.297\n",
      "Epoch 8 loss: 0.297\n",
      "[0.06046295166015625, 0.3973788022994995, 0.18847349286079407, 0.11059051752090454, 0.018546469509601593, 0.018334243446588516, 0.02295161969959736, 0.021501073613762856, 0.0672396719455719, 0.09452103823423386]\n",
      "Evaluating\n",
      "Score: 0.450\n",
      "Iteration 2\n",
      "Getting self-play records\n",
      "  Generated game 4000\n",
      "  Generated game 8000\n",
      "  Generated game 12000\n",
      "  Generated game 16000\n",
      "  Generated game 20000\n",
      "Training\n",
      "Epoch 0 loss: 0.291\n",
      "Epoch 2 loss: 0.291\n",
      "Epoch 4 loss: 0.291\n",
      "Epoch 6 loss: 0.291\n",
      "Epoch 8 loss: 0.291\n",
      "[0.054394301027059555, 0.43219253420829773, 0.18895360827445984, 0.10706517100334167, 0.01480053924024105, 0.014507905580103397, 0.01855200156569481, 0.01720096729695797, 0.061189595609903336, 0.09114336222410202]\n",
      "Evaluating\n",
      "Score: 0.475\n",
      "Iteration 3\n",
      "Getting self-play records\n",
      "  Generated game 4000\n",
      "  Generated game 8000\n",
      "  Generated game 12000\n",
      "  Generated game 16000\n",
      "  Generated game 20000\n",
      "Training\n",
      "Epoch 0 loss: 0.290\n",
      "Epoch 2 loss: 0.290\n",
      "Epoch 4 loss: 0.290\n",
      "Epoch 6 loss: 0.290\n",
      "Epoch 8 loss: 0.290\n",
      "[0.052763231098651886, 0.44166529178619385, 0.18881189823150635, 0.10598177462816238, 0.01389168482273817, 0.013584463857114315, 0.01747271604835987, 0.016153795644640923, 0.059542760252952576, 0.09013232588768005]\n",
      "Evaluating\n",
      "Score: 0.455\n",
      "Iteration 4\n",
      "Getting self-play records\n",
      "  Generated game 4000\n",
      "  Generated game 8000\n",
      "  Generated game 12000\n",
      "  Generated game 16000\n",
      "  Generated game 20000\n",
      "Training\n",
      "Epoch 0 loss: 0.289\n",
      "Epoch 2 loss: 0.289\n",
      "Epoch 4 loss: 0.289\n",
      "Epoch 6 loss: 0.289\n",
      "Epoch 8 loss: 0.289\n",
      "[0.05077497288584709, 0.45345965027809143, 0.18865245580673218, 0.10444989800453186, 0.01283335778862238, 0.012522057630121708, 0.016225652769207954, 0.01495341956615448, 0.057519834488630295, 0.08860862255096436]\n",
      "Evaluating\n",
      "Score: 0.440\n",
      "Iteration 5\n",
      "Getting self-play records\n",
      "  Generated game 4000\n",
      "  Generated game 8000\n",
      "  Generated game 12000\n",
      "  Generated game 16000\n",
      "  Generated game 20000\n",
      "Training\n",
      "Epoch 0 loss: 0.288\n",
      "Epoch 2 loss: 0.288\n",
      "Epoch 4 loss: 0.288\n",
      "Epoch 6 loss: 0.288\n",
      "Epoch 8 loss: 0.288\n",
      "[0.049275681376457214, 0.46173930168151855, 0.187681645154953, 0.10381028056144714, 0.012078219093382359, 0.011724096722900867, 0.015264980494976044, 0.014009322971105576, 0.05600230023264885, 0.08841414749622345]\n",
      "Evaluating\n",
      "Score: 0.445\n",
      "Iteration 6\n",
      "Getting self-play records\n",
      "  Generated game 4000\n",
      "  Generated game 8000\n",
      "  Generated game 12000\n",
      "  Generated game 16000\n",
      "  Generated game 20000\n",
      "Training\n",
      "Epoch 0 loss: 0.288\n",
      "Epoch 2 loss: 0.288\n",
      "Epoch 4 loss: 0.288\n",
      "Epoch 6 loss: 0.288\n",
      "Epoch 8 loss: 0.288\n",
      "[0.04883427545428276, 0.46457618474960327, 0.18778666853904724, 0.10329248011112213, 0.01185751985758543, 0.011514534242451191, 0.015020606108009815, 0.013781329616904259, 0.055545009672641754, 0.08779135346412659]\n",
      "Evaluating\n",
      "Score: 0.450\n",
      "Iteration 7\n",
      "Getting self-play records\n",
      "  Generated game 4000\n",
      "  Generated game 8000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-29f7e41eb646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_SELF_GAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_GRAPH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mrecords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rollouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNETWORK_WEIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNUM_SELF_GAMES\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNUM_SELF_GAMES\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  Generated game %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-161-ecd438c396b8>\u001b[0m in \u001b[0;36mself_play\u001b[0;34m(G, net, num_rollouts, net_weight)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpossibilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_moves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-156-1470d390899e>\u001b[0m in \u001b[0;36mdo_rollout\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mleaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-156-1470d390899e>\u001b[0m in \u001b[0;36m_expand\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mall_priors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mall_priors\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_priors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_priors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvertices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-ddde140885d0>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/columbia-deep-learning-project/env/lib/python3.7/site-packages/torch_geometric/utils/convert.py\u001b[0m in \u001b[0;36mfrom_networkx\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_node_labels_to_integers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_directed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_directed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/columbia-deep-learning-project/env/lib/python3.7/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36mto_directed\u001b[0;34m(self, as_view)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_graph_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;31m# deepcopy when not a view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m         \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/columbia-deep-learning-project/env/lib/python3.7/site-packages/networkx/classes/digraph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, incoming_graph_data, **attr)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \"\"\"\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincoming_graph_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \"\"\"Initialize a graph with edges, name, or graph attributes.\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "TRAIN_GRAPH_SIZE = 10\n",
    "TEST_GRAPH_SIZE = 10\n",
    "\n",
    "NUM_ITERS = 10\n",
    "NUM_SELF_GAMES = 20000\n",
    "NUM_EPOCHS = 10\n",
    "NUM_EVAL_SIMS = 200\n",
    "\n",
    "NETWORK_WEIGHT = 1\n",
    "\n",
    "scores = []\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr = 0.001, momentum=0.9, nesterov=True)\n",
    "criterion=torch.nn.BCELoss()\n",
    "\n",
    "test_graph = generate_graph(10)\n",
    "test_tree = MCTS()\n",
    "test_board = VertexCoverInstance(test_graph)\n",
    "moves = 0\n",
    "for _ in range(160):\n",
    "    test_tree.do_rollout(test_board)\n",
    "children = test_tree.children[test_board]\n",
    "policy = list(map(lambda n: test_tree.N[n]/(test_tree.N[test_board]-1), test_tree.children[test_board]))\n",
    "print(test_board.possible_moves)\n",
    "print(policy)\n",
    "\n",
    "for i in range(NUM_ITERS):\n",
    "    #print(list(net.parameters()))\n",
    "    print('Iteration %d' % (i))\n",
    "    \n",
    "    # get self-play records\n",
    "    print(\"Getting self-play records\")\n",
    "    records = []\n",
    "    for i in range(NUM_SELF_GAMES):\n",
    "        A = generate_graph(TRAIN_GRAPH_SIZE)\n",
    "        records.extend(self_play(A, net, num_rollouts = 50, net_weight = NETWORK_WEIGHT))\n",
    "        if i % (NUM_SELF_GAMES/5) == (NUM_SELF_GAMES/5)-1:\n",
    "            print(\"  Generated game %d\" % (i+1))\n",
    "    \n",
    "    # training\n",
    "    print(\"Training\")\n",
    "    data_loader = DataLoader(records, batch_size=256, shuffle=True)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch in data_loader:\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(batch).reshape(batch.y.shape)\n",
    "            targets = batch.y\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            #print(net.conv1.weight)\n",
    "\n",
    "        # print statistics\n",
    "        if epoch % 2 == 0:\n",
    "            print('Epoch %d loss: %.3f' %\n",
    "                  (epoch, total_loss / num_batches))\n",
    "    \n",
    "    print(net.predict(test_board))\n",
    "    print(\"Evaluating\")\n",
    "    count_success = 0\n",
    "    for x in range(NUM_EVAL_SIMS):\n",
    "        A = generate_graph(TEST_GRAPH_SIZE)\n",
    "        #vc = play_game_AZ(A, net,display=False, net_weight = NETWORK_WEIGHT)\n",
    "        vc = play_game_net(A, net,display=False)\n",
    "        opt = show_optimal_vc(A, display=False)\n",
    "        if len(vc)<=len(opt)*1.1:\n",
    "            count_success=count_success+1\n",
    "    print('Score: %.3f' % (count_success / NUM_EVAL_SIMS))\n",
    "    scores.append(count_success / NUM_EVAL_SIMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.close()\n",
    "plt.plot(scores)\n",
    "plt.ylabel('Performance')\n",
    "plt.xlabel('Self Play / Training Iterations')\n",
    "plt.title('Percentage of Good AlphaMinVertex Approximations during Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "G = generate_graph(20)\n",
    "vc = play_game_net(G, net)\n",
    "print()\n",
    "opt = show_optimal_vc(G)\n",
    "print('Finished in {} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
